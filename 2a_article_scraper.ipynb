{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article PDFs download\n",
    "\n",
    "The AK API provided a list of articles (publications_eng.csv) with a list of source links and the DOI for some articles  \n",
    "\n",
    "The below code uses the provided information (URLs and DOI) to search for and try to download a PDF of each of the identified articles\n",
    "\n",
    "We identified 3 methods for downloading PDFs but only used method 1 and 2 as method 3 was the least reliable (had the greatest potential that files other than the sought after article would be downloaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from requests import get \n",
    "from bs4 import BeautifulSoup\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import re, json, ast, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory that save found PDFs to\n",
    "base_dir = 'papers'\n",
    "\n",
    "#load publication information\n",
    "publications = pd.read_csv('data_files/publications_eng.csv', encoding = \"ISO-8859-1\")\n",
    "n_articles = publications.shape[0]\n",
    "print('number of articles to search: ', n_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#note all unique paper ids\n",
    "paper_ids = list(publications['paper_id'].unique())\n",
    "found_papers = {}\n",
    "for i in paper_ids:\n",
    "    found_papers[i] = 0\n",
    "    \n",
    "#in cases that need to run the script multiple times\n",
    "#get information on which papers have pdf file for already\n",
    "rootdir = 'papers_final'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "#             paper_id = file.split('.')[0]\n",
    "            paper_id = subdir.split('/')[-1]\n",
    "            found_papers[int(paper_id)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_pdf(url, base_dir, name):\n",
    "    \"\"\"\n",
    "    take url and make request, if retrieved content is pdf, save to base_dir as <name>.pdf\n",
    "    indicate whether pdf has been found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        content = get(url)\n",
    "        if content.status_code==200 and content.headers['content-type']=='application/pdf':\n",
    "            with open(os.path.join(base_dir, str(name) +'.pdf'), 'wb') as pdf:\n",
    "                pdf.write(content.content)\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Retrieve PDFs using AK API provided URLs\n",
    "\n",
    "Visit each URL and if it is a PDF, download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_link(url):\n",
    "    \"\"\"\n",
    "    takes in url\n",
    "    if url is pointing to one of the well known open source platforms but does not end in 'pdf' - adapt it\n",
    "    \n",
    "    returns link to use in request\n",
    "    \"\"\"\n",
    "    \n",
    "    #if url ends in 'pdf' - leave as is\n",
    "    if url[-3:].lower() == 'pdf':\n",
    "        link = url\n",
    "\n",
    "    #if url is pointing to one of the well known open source platforms \n",
    "    #but does not end in 'pdf' -- adapt\n",
    "    elif 'arxiv' in url:    \n",
    "        #remove queries sometimes present at the end of the URL\n",
    "        no_query = url.split('?')[0]\n",
    "        #get article arxiv ID\n",
    "        article_id = no_query.split('/')[-1]    \n",
    "        #construct link\n",
    "        link = 'https://arxiv.org/pdf/' + str(article_id) + '.pdf'\n",
    "\n",
    "    elif 'peerj' in url:\n",
    "        link = url[:-1] + '.pdf'\n",
    "\n",
    "    elif 'frontiers' in url:\n",
    "        link = url[:-4] + 'pdf'\n",
    "\n",
    "    elif 'plos' in url:\n",
    "        components = url.split('?')\n",
    "        if len(components)==2:\n",
    "            start = \"/\".join(components[0].split('/'))\n",
    "            if start[-1] != 'article':\n",
    "                link = \"/\".join(components[0].split('/')[:-1]) + \"/file?\" + components[1] + '&type=printable'\n",
    "            else: link = components[0] + \"/file?\" + components[1] + '&type=printable'\n",
    "        else:\n",
    "            link = url\n",
    "\n",
    "    elif 'rsos' in url:\n",
    "        link = url + '.full.pdf'\n",
    "\n",
    "    #otherwise leave as is\n",
    "    else:\n",
    "        link = url\n",
    "        \n",
    "    return link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index, row in publications.iterrows():\n",
    "    \n",
    "    urls = row['url_0':'url_19']\n",
    "    paper_id = row['paper_id']\n",
    "        \n",
    "    for i in urls:\n",
    "\n",
    "        #check that i is a string rather than NaN and that paper has not been found yet\n",
    "        if type(i) != float and found_papers[paper_id] == 0:\n",
    "\n",
    "            #form URL to request\n",
    "            link = get_link(i)\n",
    "\n",
    "            #try to extract pdf - check if successful\n",
    "            found = extract_pdf(link, base_dir, paper_id)\n",
    "            if found:\n",
    "                found_papers[paper_id] = 1\n",
    "\n",
    "    #keep track of progress\n",
    "    if index != 0 and index%100 == 0:\n",
    "        print(str(round(index/n_articles, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Search for open access article using article DOI and the oaDOI API\n",
    "\n",
    "the oaDOI provides a database of links to open access versions of an article which can be retrieved using the article DOI  \n",
    "We use it to look for further links to articles that have not yet been retrieved (where the DOI is known)  \n",
    "\n",
    "https://oadoi.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_bytes(bytes_string):\n",
    "    \"\"\"\n",
    "    takes dictionary represented as bytes string\n",
    "    returns dictionary\n",
    "    \"\"\"\n",
    "    data = \"\".join([word for word in bytes_string.split() if word != \"\\n\"])\n",
    "    repls = ('true', 'True'), ('false', 'False'), ('null', '0')\n",
    "    data = reduce(lambda a, kv: a.replace(*kv), repls, data)\n",
    "    \n",
    "    return ast.literal_eval(data)\n",
    "\n",
    "def extract_oadoi_data(doi):\n",
    "    \"\"\"\n",
    "    takes doi\n",
    "    returns list of urls provided by oaDOI\n",
    "    \"\"\"\n",
    "    url = 'https://api.oadoi.org/v2/'\n",
    "    link = url + doi\n",
    "    urls = []\n",
    "    \n",
    "    try:\n",
    "        content = get(link)\n",
    "        data = content.content.decode()\n",
    "        data_dict = decode_bytes(data)\n",
    "\n",
    "        if 'oa_locations' in data_dict.keys():\n",
    "            for i in data_dict['oa_locations']:\n",
    "                    urls.append(i['url'])\n",
    "    except:\n",
    "        #if get internal server error\n",
    "        pass\n",
    "    \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index, row in publications.iterrows():\n",
    "    paper_id = row['paper_id']\n",
    "    doi = row['doi']\n",
    "    \n",
    "    #check paper has not been found yet + has a valid doi\n",
    "    if found_papers[paper_id] == 0 and type(doi) == str:\n",
    "\n",
    "        urls = extract_oadoi_data(doi)\n",
    "        \n",
    "        if len(urls) > 0:\n",
    "            for i in urls:\n",
    "                if found_papers[paper_id] == 0:\n",
    "                    found = extract_pdf(i, base_dir, paper_id)\n",
    "                    if found:\n",
    "                        found_papers[paper_id] = 1\n",
    "                            \n",
    "    if index != 0 and index%100 == 0:\n",
    "        print(str(round(index/n_articles, 2)) + \"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Search for links to PDF on article linked webpage \n",
    "\n",
    "For the articles that have not been downloaded, visit the AK API provided URLs, search for links to PDFs within the retrieved HTML and try to download those files    \n",
    "\n",
    "NOTE: there is no guarantee the downloaded file is the searched for paper as compared to e.g., supplementary information files or repository related documents or even another paper entirely  \n",
    "\n",
    "It might therefore be preferred to check the downloaded files for content if using this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    content = get(url)\n",
    "    if content.status_code == 200:\n",
    "        return content.text\n",
    "\n",
    "def get_all_links(content):\n",
    "    soup = BeautifulSoup(content)\n",
    "    links = []\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        links.append(link.get('href'))\n",
    "    return links\n",
    "\n",
    "def get_pdf(url, base_dir, paper_id):\n",
    "    content = get_page(url)\n",
    "    links = get_all_links(content)\n",
    "    n_pdfs = 0\n",
    "    \n",
    "    for link in links:\n",
    "        if 'pdf' in link:\n",
    "            #some pages have 2 links to pdf - e.g. common for scirate.com -- avoid duplication\n",
    "            if n_pdfs == 0:\n",
    "                found = extract_pdf(link, base_dir, paper_id)\n",
    "                if found:\n",
    "                    n_pdfs+= 1  \n",
    "                    \n",
    "    if n_pdfs==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in publications.iterrows():\n",
    "\n",
    "    paper_id = row['paper_id']\n",
    "    urls = row['url_0':'url_19']\n",
    "    \n",
    "    if found_papers[paper_id] == 0:\n",
    "                \n",
    "        for i in urls:\n",
    "            \n",
    "            if type(i) != float and found_papers[paper_id] == 0:\n",
    "                                                \n",
    "                try:\n",
    "                    found = get_pdf(i, base_dir, paper_id)\n",
    "                    if found:\n",
    "                        found_papers[paper_id] = 1\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "    if index != 0 and index%100 == 0:\n",
    "        print(str(round(index/n_articles, 2)) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
