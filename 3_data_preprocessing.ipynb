{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "\n",
    "- publications and fellows related data are stored in separate csv files  \n",
    "- all downloaded article texts are in separate text files  \n",
    "\n",
    "- here we clean and combine all data into a new csv for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data manipulation and organisation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#data pre-processing\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from difflib import SequenceMatcher\n",
    "from collections import Counter\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "#other\n",
    "import os, random, json, re, unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Turing fellows listed on website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    page = urlopen(url) \n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "url = 'https://www.turing.ac.uk/turing-fellows/'\n",
    "soup = get_page(url)\n",
    "all_fellows = soup.find_all('div', attrs={'class':'fellow-card'})\n",
    "\n",
    "uni_counts = []\n",
    "for i in all_fellows:\n",
    "    #get affiliation - saved in class names\n",
    "    attributes = i.get('class')\n",
    "    university = attributes[1]\n",
    "    uni_counts.append(university)\n",
    "uni_counts = Counter(uni_counts)\n",
    "\n",
    "num_web_fellows = len(all_fellows) - uni_counts['Collaborating-Fellows']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths to data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "publications_eng.csv : contains a list of articles (one per row) and associated information including researcher's name (version returned by AK API), researcher ID, paper ID, title, abstract. These are the articles that Academic Knowledge API returned for each fellow - we do not have a full version of the article for each of these    \n",
    "\n",
    "turing_AK_IDs.csv : links each 'author_id' to version of fellow name we accept (e.g. AK API might return both 't lyons' and 'terry lyons', we want to store both as 'terry lyons', not as separate researchers)  \n",
    "\n",
    "papers_final : directory that contains all found article .pdf and converted .txt files (each saved in a directory named after the paper ID)   \n",
    "\n",
    "All information can be matched using author (researcher) and paper IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rootdir = 'papers'\n",
    "data_dir = 'data_files/'\n",
    "\n",
    "fellows = 'turing_AK_IDs.csv'\n",
    "article_info = 'publications_eng.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Upload, clean and finalise dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, combine and check files - general article and Turing fellows information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and drop empty columns\n",
    "publications = pd.read_csv(data_dir + article_info, encoding = \"ISO-8859-1\").dropna(axis=1, how='all')\n",
    "fellow_ids = pd.read_csv(data_dir + fellows) \n",
    "\n",
    "#make university names constant\n",
    "unis = {'Cambridge University': 'Cambridge', 'University of Cambridge': 'Cambridge',\n",
    "        'Edinburgh University': 'Edinburgh', 'University of Edinburgh': 'Edinburgh',\n",
    "        'Oxford University':'Oxford', 'University of Oxford':'Oxford',\n",
    "        'University College London':'UCL',\n",
    "        'Warwick University': 'Warwick', 'University of Warwick':'Warwick'}\n",
    "\n",
    "#get ids\n",
    "ids = {}\n",
    "for index, row in fellow_ids.iterrows():\n",
    "    name = row['name']\n",
    "    author_ids = row['id_1':'id_3']\n",
    "    for author_id in author_ids:\n",
    "        if not pd.isnull(author_id):\n",
    "            ids[author_id] = name\n",
    "                        \n",
    "publications = publications.rename(columns={'standard_name': 'full_name'})         \n",
    "publications['current_uni'] = publications['full_name'].apply(lambda x: unis[fellow_ids.loc[fellow_ids['name']==x]['uni'].values[0]])\n",
    "publications['ak_keywords'] = (pd.Series(publications.loc[:,'keyword_0':'keyword_26'].values.tolist())\n",
    "                               .apply(lambda row: '; '.join(e for e in row if not pd.isnull(e))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how many fellows have associated with each university - compare against how many are on website\n",
    "uni_names = publications['current_uni'].unique()\n",
    "uni_fellows = publications.groupby('current_uni')['full_name'].unique()\n",
    "\n",
    "for i in range(len(uni_names)):\n",
    "    print(uni_names[i] + \": \" + str(len(uni_fellows[i])) + ' of ' + str(uni_counts[uni_names[i]]) + \" fellows\")\n",
    "    #print(random.sample(set(uni_fellows[i]), 2))\n",
    "    \n",
    "num_fellows = len(publications['full_name'].unique())\n",
    "print('\\nExpect {0} fellows overall, have: '.format(num_web_fellows) + str(num_fellows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note which articles have a .txt file for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#note all unique paper ids\n",
    "paper_ids = list(publications['paper_id'].unique())\n",
    "found_pdf = {}\n",
    "for i in paper_ids:\n",
    "    found_pdf[i] = 0\n",
    "    \n",
    "#get information on which papers have text file for\n",
    "count = 0\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    check = False\n",
    "    \n",
    "    #search through files in each directory for .txt file\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            check = True\n",
    "            paper_id = subdir.split('/')[-1]\n",
    "            found_pdf[int(paper_id)] = 1\n",
    "            count += 1\n",
    "            \n",
    "    #if no .txt file was found (pdf convert failed) - print directory\n",
    "    if not check and len(subdir.split('/'))==2:\n",
    "        print('No text file in the {0} directory'.format(subdir.split('/')[1]))\n",
    "            \n",
    "publications['found_pdf'] = publications['paper_id'].apply(lambda x: found_pdf[x])\n",
    "\n",
    "print('\\nHave {0} full articles in text format'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Check for multiple instances of same paper (same/similar paper title, same fellow but saved under 2 unique IDs) - remove duplicates\n",
    "\n",
    "For each fellow check all the found paper titles agains each other for similarity  \n",
    "\n",
    "If the titles are the same or sufficiently similar AND PDFs for more than 1 have been found then choose 1 (randomly) and delete the others  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_same_titles(publications, titles, name):\n",
    "    \"\"\"\n",
    "    function that checks for multiple versions of same title as well as for similar titles in a list of titles\n",
    "    expects publications df with 'paper_id', 'title' and 'name' information\n",
    "    \n",
    "    returns list of IDs identifying the same papers - can choose one from this list to keep\n",
    "    also returns remaining titles if want to further check these for similarity\n",
    "    \"\"\"\n",
    "    to_remove = []\n",
    "    \n",
    "    #count how many times each title occurs in collection\n",
    "    counter = Counter(titles)\n",
    "    to_check = []\n",
    "    \n",
    "    #if title recorded multiple times --> counter[title] > 1\n",
    "    for title in counter.keys():\n",
    "        \n",
    "        if counter[title] == 1:\n",
    "            to_check.append(title)\n",
    "        else:\n",
    "            idx = publications.loc[(publications['title']==title) & (publications['full_name'] == name)]['paper_id'].values\n",
    "            to_remove.append(list(idx))\n",
    "            \n",
    "    return to_remove, to_check\n",
    "            \n",
    "def get_similar_titles(publications, titles, name, threshold):\n",
    "    \"\"\"\n",
    "    function that checks for for similar titles in a list of titles\n",
    "    returns list of IDs identifying the same papers - can choose one from this list to keep\n",
    "    expects publications df with 'paper_id', 'title' and 'name' information\n",
    "    can specify similarity threshold - this is a ratio (above what score to note similar looking titles papers for removal)\n",
    "    \"\"\"\n",
    "    to_remove = []\n",
    "    marked = []\n",
    "    \n",
    "    #loop through titles and check for similarity\n",
    "    for i in range(len(titles)):\n",
    "        \n",
    "        #get paper title and save ID\n",
    "        title = titles[i]\n",
    "        paper_id = publications.loc[publications['title']==title]['paper_id'].values[0]\n",
    "        similar = [paper_id]\n",
    "        \n",
    "        #only search through papers that come after current\n",
    "        for title_2 in titles[i+1:]:\n",
    "            \n",
    "            paper_id = publications.loc[publications['title']==title_2]['paper_id'].values[0]\n",
    "            \n",
    "            #check paper has not already been included in a to_remove set\n",
    "            if paper_id not in marked:\n",
    "            \n",
    "                score = SequenceMatcher(None, title, title_2).ratio()\n",
    "                if score >  threshold :\n",
    "                    similar.append(paper_id)\n",
    "                    \n",
    "        if len(similar) > 1:\n",
    "            to_remove.append(similar)     \n",
    "            marked.extend(similar)\n",
    "    \n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_keep = {}\n",
    "\n",
    "#check each fellow, one at a time\n",
    "for  name in publications['full_name'].unique():\n",
    "    \n",
    "    #get all paper titles that have a pdf for\n",
    "    titles = publications.loc[(publications['full_name'] == name) & (publications['found_pdf']==1)]['title'].values\n",
    "    \n",
    "    #check for duplicates and save paper_ids if found any\n",
    "    same_papers, to_check = get_same_titles(publications, titles, name)\n",
    "    if len(same_papers) > 0:\n",
    "        for id_set in same_papers:\n",
    "            #shuffle paper id order,then mark first item for keeping [0] and rest for removal \n",
    "            random.shuffle(id_set)\n",
    "            to_keep[id_set[0]] = 1\n",
    "            for i in id_set[1:]:\n",
    "                to_keep[i] = 0\n",
    "\n",
    "    #check for similar paper titles - pick 1 to keep and mark remaining for removal\n",
    "    similar_papers = get_similar_titles(publications, to_check, name, .85)\n",
    "    if len(similar_papers) > 0:\n",
    "        for id_set in similar_papers:\n",
    "            random.shuffle(id_set)\n",
    "            to_keep[id_set[0]] = 1\n",
    "            for i in id_set[1:]:\n",
    "                to_keep[i] = 0\n",
    "            \n",
    "#mark the remaining papers as to keep\n",
    "for paper_id in publications['paper_id'].unique():\n",
    "    if paper_id not in to_keep.keys():\n",
    "        to_keep[paper_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications['to_keep'] = publications['paper_id'].apply(lambda x: to_keep[x])\n",
    "\n",
    "print('Excluding {0} repeats'.format(len(publications.loc[publications['to_keep'] == 0].values)))\n",
    "\n",
    "publications = publications.loc[publications['to_keep'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Count number of abstracts and pdfs obtained per researcher - note researchers that do not meet criteria\n",
    "\n",
    "These are researchers for whom we either found fewer than 5 PDFs OR we found less than 20% of the originally identified papers  \n",
    "This will be shown in the visualisation   \n",
    "Save in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_pdfs = pd.concat([publications.groupby('full_name')['paper_id'].count(), publications.groupby('full_name')['found_pdf'].sum()], axis=1)\n",
    "num_pdfs['proportion_found'] = num_pdfs['found_pdf']/num_pdfs['paper_id']\n",
    "num_pdfs = num_pdfs.rename(columns={'paper_id':'article_count', 'found_pdf':'full_article_count'})\n",
    "num_pdfs['name'] = num_pdfs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_exclude = {}\n",
    "author_info = {}\n",
    "\n",
    "for index, row in num_pdfs.iterrows():\n",
    "    \n",
    "    fellow = row['name']\n",
    "    uni = publications.loc[publications['full_name']==fellow]['current_uni'].values[0]\n",
    "    author_info[fellow] = [fellow.title(), uni]\n",
    "    \n",
    "    if row['full_article_count'] < 5: # or (row['full_article_count'] >= 5 and row['proportion_found'] < .2):\n",
    "        exclude = 1\n",
    "    else:\n",
    "        exclude = 0\n",
    "        \n",
    "    to_exclude[index] = exclude\n",
    "    author_info[fellow].append(exclude)\n",
    "    \n",
    "num_pdfs['to_exclude'] = num_pdfs['name'].apply(lambda x: to_exclude[x])\n",
    "\n",
    "num_pdfs.loc[num_pdfs['to_exclude']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save author information to use in visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('visualisation/author_info_final.json', 'w') as fp:\n",
    "    json.dump(author_info, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Clean text data (and extract keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up for pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define stop words - usual + mention of researcher names and affiliations\n",
    "stop_words = set(stopwords.words('english'))|set(publications['current_uni'].unique())|set(publications['full_name'].unique())\n",
    "\n",
    "#keep only letters and spaces i.e. remove digits, special characters, punctuation\n",
    "pattern = re.compile(\"[^A-Za-z\\s]+\")\n",
    "\n",
    "#set up stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(line):\n",
    "    \n",
    "    #replace '/' and '-' with spaces rather than remove\n",
    "    line = line.replace(\"/\", \" \").replace(\"-\", \" \")\n",
    "    \n",
    "    #remove digits and special characters\n",
    "    line = pattern.sub('', line)\n",
    "\n",
    "    #remove double spaces\n",
    "    line = line.replace(\"  \", \" \")\n",
    "    \n",
    "    #remove end of line white-space\n",
    "    line = line.strip()\n",
    "\n",
    "    #remove stop words and stem\n",
    "    #longest word in major dictionary has 45 letters - remove anything longer\n",
    "    line = ' '.join([stemmer.stem(word) for word in line.split() if word not in stop_words and len(word) <= 45])\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean all retrieved .txt files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers = {}\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            paper_id = subdir.split('/')[-1]\n",
    "\n",
    "            if int(paper_id) in publications['paper_id'].unique():\n",
    "                path = os.path.join(subdir, file)\n",
    "                my_file = open(path, 'r')\n",
    "                text = \"\"           \n",
    "\n",
    "                for line in my_file:\n",
    "                    text += clean_text(line).lower() \n",
    "                    text += \" \"\n",
    "\n",
    "                papers[paper_id] = text\n",
    "                my_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save full text + document lengths alongside other article information in new df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications['full_text'] = publications['paper_id'].apply(lambda x: papers[str(x)] if str(x) in papers.keys() else \"\")\n",
    "\n",
    "to_analyse = publications.loc[publications['full_text'] != \"\"]\n",
    "to_analyse['doc_length'] = to_analyse['full_text'].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "to_analyse = to_analyse.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Check lengths of documents and remove outliers (texts that are too short/long)\n",
    "\n",
    "NOTE: these word counts are after stop words have been removed (but before low and high frequency words have been removed)  \n",
    "\n",
    "At the moment we are removing texts that are less than 500 words and texts that are longer than 20 000 words  \n",
    "Clearly these thresholds are fairly arbitrary but very lenient and based on what we would expect from an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_analyse['doc_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = to_analyse['doc_length']\n",
    "plt.hist(doc_lengths, bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of articles that have fewer than 500 words: ', to_analyse.loc[to_analyse['doc_length'] < 500].count().values[0])\n",
    "print('number of articles that are more than 20 000 words long: ', to_analyse.loc[to_analyse['doc_length'] > 20000].count().values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_analyse = to_analyse.loc[(to_analyse['doc_length'] <= 20000) & (to_analyse['doc_length'] >= 500)]\n",
    "to_analyse = to_analyse.reset_index(drop=True)\n",
    "\n",
    "print('{0[0]} articles remain for analysis'.format(to_analyse.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split data so that can save into smaller files in chunks\n",
    "split = int(to_analyse.shape[0]/2)\n",
    "\n",
    "first_half = to_analyse.iloc[:split]\n",
    "second_half = to_analyse.iloc[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Save data in a new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select columns of interest\n",
    "cols = ['author_id'] + ['full_name'] + ['current_uni'] + list(to_analyse.loc[:, 'paper_id':'title']) + list(to_analyse.loc[:,'full_text':'doc_length']) + ['ak_keywords']\n",
    "\n",
    "#save file - full dataset but also split into halfs (i.e. smaller file size)\n",
    "final_dataset = to_analyse[cols]\n",
    "final_dataset.to_csv('data_files/final_dataset_full.csv', index = False)\n",
    "first_half.to_csv('data_files/final_dataset_1.csv', index=False)\n",
    "second_half.to_csv('data_files/final_dataset_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
