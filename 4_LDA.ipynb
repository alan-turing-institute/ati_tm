{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling (LDA) of Turing Institute publications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data manipulation and organisation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#topic modelling\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#visualisations\n",
    "import pyLDAvis\n",
    "from pyLDAvis import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#other\n",
    "import random, pkg_resources, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Require sklearn version 0.19, have: ' + pkg_resources.get_distribution(\"scikit-learn\").version)\n",
    "print(\"If have lower version, need to change 'n_components' to 'n_topics' when calling LatentDirichletAllocation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##might need to download nltk corpora and packages\n",
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Load data and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check if have full dataset in 1 file and load, otherwise look for full dataset in 2 files and combine\n",
    "if os.path.isfile('data_files/final_dataset_full.csv'):\n",
    "    publications_data = pd.read_csv('data_files/final_dataset_full.csv')\n",
    "else:\n",
    "    publications_1 = pd.read_csv('data_files/final_dataset_1.csv')\n",
    "    publications_2 = pd.read_csv('data_files/final_dataset_2.csv')\n",
    "    publications_data = pd.concat([publications_1, publications_2])\n",
    "\n",
    "#rename some columns\n",
    "publications_data = publications_data.rename(columns={'full_name': 'name', 'current_uni': 'uni'})\n",
    "    \n",
    "publications_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Dataset contains {0[0]} article records'.format(publications_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check how many fellows have associated with each university\n",
    "uni_names = publications_data['uni'].unique()\n",
    "uni_fellows = publications_data.groupby('uni')['name'].unique()\n",
    "\n",
    "for i in range(len(uni_names)):\n",
    "    print(uni_names[i] + \": \" + str(len(uni_fellows[i])) + \" fellows\")\n",
    "    #print(random.sample(set(uni_fellows[i]), 2))\n",
    "    \n",
    "num_fellows = len(publications_data['name'].unique())\n",
    "print('\\nExcpect 108 fellows overall, have: ' + str(num_fellows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Note instances of multiple Turing fellows associated with 1 paper - remove duplicates based on paper ID (but keep note of all authors to attribute later)\n",
    "\n",
    "paper_id_to_authors dictionary keys correspond to all unique paper ids  \n",
    "dictionary value is a list of authors associated with that paper (most instances have 1 but in some cases have multiple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paper_id_to_authors = {}\n",
    "for idx, row in publications_data.iterrows():\n",
    "    if row['paper_id'] not in paper_id_to_authors.keys():\n",
    "        paper_id_to_authors[row['paper_id']] = [row['name']]\n",
    "    else:\n",
    "        paper_id_to_authors[row['paper_id']].append(row['name'])\n",
    "        \n",
    "#drop duplicates\n",
    "publications_data = publications_data.drop_duplicates(subset = 'paper_id')\n",
    "\n",
    "#relabel index numbers\n",
    "publications_data = publications_data.reset_index(drop=True)\n",
    "\n",
    "print('Dataset contains {0[0]} unique articles'.format(publications_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: LDA\n",
    "\n",
    "we are looking for 25 topics with both priors set to .01  \n",
    "-- these values are consistent with the LDA parameter exploration results (although other values could be also used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create document-term matrix from text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1: create vector representation of vocabulary\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "\n",
    "#create document_term_matrix\n",
    "dtm = vectorizer.fit_transform(publications_data['full_text'].values.astype('U'))\n",
    "\n",
    "#retrieve word names at each vocabulary position\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 25\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = n_topics, \n",
    "                                max_iter = 50, #default is 10 - if threshold condition is met earlier, updates stop\n",
    "                                learning_method = 'online',\n",
    "                                random_state = 0,\n",
    "                                doc_topic_prior = .01, \n",
    "                                topic_word_prior = .01) \n",
    "\n",
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise topics (LDAvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook\n",
    "prepared_data = pyLDAvis.sklearn.prepare(lda, dtm, vectorizer)\n",
    "pyLDAvis.display(prepared_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Organise LDA outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve topic_term and document_topic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_with_names(data, index_name, columns_name):\n",
    "    if type(data) == pd.DataFrame:\n",
    "        #we want our index to be numbered\n",
    "        df = pd.DataFrame(data.values)\n",
    "    else:\n",
    "        df = pd.DataFrame(data)\n",
    "    df.index.name = index_name\n",
    "    df.columns.name = columns_name\n",
    "    return df\n",
    "\n",
    "def series_with_name(data, name):\n",
    "    if type(data) == pd.Series:\n",
    "        data.name = name\n",
    "        #ensures a numeric index\n",
    "        return data.reset_index()[name]\n",
    "    else:\n",
    "        return pd.Series(data, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_term_dists = lda.components_ / lda.components_.sum(axis=1)[:, None]\n",
    "doc_topic_dists = lda.transform(dtm)\n",
    "\n",
    "topic_term_dists = df_with_names(topic_term_dists, 'topic', 'term')\n",
    "doc_topic_dists  = df_with_names(doc_topic_dists, 'doc', 'topic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order topics in both distributions to match LDAvis output\n",
    "this is ordering topics by prevalence of topic in the entire corpus  \n",
    "it makes exploring results easier as we can easily compare to above visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topic_order(dtm, doc_topic_dists):\n",
    "    \"\"\"\n",
    "    function which returns same topic order as LDAvis\n",
    "    \"\"\"\n",
    "    doc_lengths      = series_with_name(dtm.sum(axis=1).getA1(), 'doc_length')\n",
    "    topic_freq       = (doc_topic_dists.T * doc_lengths).T.sum()\n",
    "    topic_proportion = (topic_freq / topic_freq.sum()).sort_values(ascending=False)\n",
    "    \n",
    "    return topic_proportion.index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_order      = get_topic_order(dtm, doc_topic_dists)\n",
    "\n",
    "topic_term_dists = topic_term_dists.ix[topic_order]\n",
    "doc_topic_dists  = doc_topic_dists[topic_order]\n",
    "\n",
    "print('topic_term and doc_topic distributions have same topic order: ', set(topic_term_dists.index.values == doc_topic_dists.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5:  Format data for visualisation\n",
    "\n",
    "### Transform document_topic distribution to fellow_topic and institute_topic information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "publications_data df:  \n",
    "each row of the df corresponds to row of same index in doc_topic_dists   \n",
    "this allows us to get paper_id for the doc_topic_distribution of any paper   \n",
    "we can then match the paper_id against known Turing fellows associated with that paper  \n",
    "\n",
    "For each fellow, we get the average of distributions over topics for their documents and multiple by 100 -- we can imagine that each author has 100 'points' and these are divided between the modelled 25 topics based on the average proportion that their articles have been assigned each topic (this is based on how many words in each document have been assigned to each topic)      \n",
    "\n",
    "Overall topic importance is the average of above topic value assignment across all fellows  - this means each fellow contributes equally to the topic importance/size evaluation (even if they overall contributed fewer papers)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract topic proportions for each author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_names = publications_data['name'].unique()\n",
    "author_topic_dists = {}\n",
    "author_counts = {}\n",
    "\n",
    "for name in author_names:\n",
    "    author_topic_dists[name] = np.zeros(n_topics)\n",
    "    author_counts[name] = 0\n",
    "    \n",
    "for index, row in publications_data.iterrows():\n",
    "    #most papers have 1 author but some have multiple so this assures paper topics are assigned to all relevant authors\n",
    "    for name in paper_id_to_authors[row['paper_id']]:\n",
    "        author_topic_dists[name] += doc_topic_dists.iloc[index]\n",
    "        author_counts[name] += 1\n",
    "\n",
    "# proportion of topics per author - for each author these sum to 1\n",
    "for name in author_topic_dists.keys():\n",
    "    if author_topic_dists[name].sum() != 0:\n",
    "        author_topic_dists[name] = author_topic_dists[name]/author_counts[name]*100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract overall institute topic \"importance\" (size) information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_importance, total = {}, 0\n",
    "for i in range(n_topics):\n",
    "    topic_importance[i] = 0\n",
    "    for name in author_topic_dists:\n",
    "        topic_importance[i] += author_topic_dists[name][i]\n",
    "    topic_importance[i] = topic_importance[i]/num_fellows\n",
    "    total += topic_importance[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all topic information in one df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "author_topic_info = pd.DataFrame.from_dict(author_topic_dists, orient = 'columns')\n",
    "topic_info = pd.DataFrame.from_dict(topic_importance, orient='index')\n",
    "merged = pd.concat([author_topic_info, topic_info], axis =1 )\n",
    "merged = merged.rename(columns={0: 'topicVal'})\n",
    "\n",
    "##order topics by topicVal (which depends on researchers) rather than importance as defined by LDAvis\n",
    "merged = merged.sort_values(['topicVal'], ascending = 0)\n",
    "#save the new order as will use this information later\n",
    "order = list(merged.index.values)\n",
    "#reset index\n",
    "merged = merged.reset_index(drop=True)\n",
    "\n",
    "print('To check whether each column sums to 100, check set of all column sums (rounded to 5 dp)')\n",
    "print('Expect: {100.0}, get:', set(np.around(merged.sum(axis = 0).values, decimals=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine topics that have assigned topic value < 1.5 (out of 100) into 1 topic that will be labeled 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_combine = merged.loc[merged['topicVal'] < 1.5]\n",
    "key_topics = merged.loc[merged['topicVal'] >= 1.5]\n",
    "\n",
    "#make note of how many topics will need to name\n",
    "num_topics_to_name = key_topics.shape[0]\n",
    "\n",
    "key_topics = key_topics.append(to_combine.sum(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last checks and save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Check topic assignments')\n",
    "print('Expect: {100.0}, get:', set(np.around(merged.sum(axis = 0).values, decimals=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_topics.to_csv(\"data.csv\", index_label='topicNum')\n",
    "print(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Determine order in which to display researchers - ordered by university, by most prevalent topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "can_shuffle = merged\n",
    "researchers_dict = {}\n",
    "num = 0\n",
    "for column in can_shuffle:\n",
    "    num += 1\n",
    "    \n",
    "    name = column\n",
    "    \n",
    "    #sort rows (documents) in descending order of probability of topic\n",
    "    topics = can_shuffle.sort_values([column], ascending = 0)\n",
    "\n",
    "    index = topics.index.values[0]\n",
    "\n",
    "    val = topics[column][index]\n",
    "    \n",
    "    if name != 'topicVal':\n",
    "        uni = publications_data.loc[publications_data['name']== name]['uni'].values[0]\n",
    "\n",
    "        if uni not in researchers_dict.keys():\n",
    "            researchers_dict[uni] = {index:[[name, val]]}\n",
    "        else:\n",
    "            if index not in researchers_dict[uni].keys():\n",
    "                researchers_dict[uni][index] = [[name, val]]\n",
    "            else:\n",
    "                researchers_dict[uni][index].append([name, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loop through universities\n",
    "researchers_order = []\n",
    "for i in sorted(researchers_dict.keys()):\n",
    "    topics = researchers_dict[i]\n",
    "    for j in sorted(topics.keys()):\n",
    "        researchers = topics[j]\n",
    "        \n",
    "        if len(researchers) == 1:\n",
    "            researchers_order.append(researchers[0][0])\n",
    "        else:\n",
    "            researchers.sort(key=lambda x: x[1], reverse=True)\n",
    "            for k in researchers:\n",
    "                researchers_order.append(k[0])\n",
    "\n",
    "print(researchers_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Topic interpretation\n",
    "\n",
    "### Extract top documents for each topic (with associated top words and titles and AK API keywords)\n",
    "\n",
    "For each topic we extract top N words associated with that topic (this is very standard and is the same as displayed in the LDAvis visualisation)  \n",
    "\n",
    "Further, for each topic we get the titles of key articles associated with that topic - this makes topics easier to interpret   \n",
    "\n",
    "Each document has been assigned some proportion of each topic (with sum of all topic proportions within document = 1). For each topic, we ordered documents based on how much of that topic they were assigned. Then we looped through the documents extracting top 10 key article titles with the condition that if a researcher already contributed 2 papers to the top 10 key titles then we skipped their subsequent papers. This ensured that the top 10 key titles contained contributions of at least 5 researchers (as compared to trying to evaluate the meaning of a topic based on articles written by only one Turing fellow). This seemed reasonable given most topics were assigned to a number of researchers.  \n",
    "\n",
    "For interpreting small topics (which might only be associated with 2 or 3 we researchers), we also kept track of the 10 top papers (with no constraints attached)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get N top words for different values of lambda (1.0, 0.6, 0.2) -- from N most frequent to N most unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_topic_freq(dtm, doc_topic_dists):\n",
    "    \"\"\"\n",
    "    same as LDAvis, define topic frequency by proportion of words assigned to topic\n",
    "    \"\"\"\n",
    "    doc_lengths = dtm.sum(axis=1).getA1()\n",
    "    topic_freq = (doc_topic_dists.T * doc_lengths).T.sum()\n",
    "    \n",
    "    return topic_freq\n",
    "    \n",
    "def get_relevance(topic_freq, topic_term_dists, lambda_):\n",
    "    \"\"\"\n",
    "    function for calculating relevance\n",
    "    if lambda_ = 1 result is same as top N mot frequent words\n",
    "    if lambda_ = 0; returns list of top unique words for each topic \n",
    "    unique = not shared by other topics\n",
    "    \"\"\"\n",
    "    term_topic_freq = (topic_term_dists.T * topic_freq).T\n",
    "    term_frequency = np.sum(term_topic_freq, axis=0)\n",
    "    term_proportion = term_frequency / term_frequency.sum()  \n",
    "    \n",
    "    log_lift = np.log(topic_term_dists / term_proportion)\n",
    "    log_ttd = np.log(topic_term_dists)\n",
    "    relevance = lambda_ * log_ttd + (1 - lambda_) * log_lift\n",
    "    \n",
    "    return relevance\n",
    "\n",
    "def get_top_words(relevance, feature_names, n_top):\n",
    "    \"\"\"\n",
    "    function to extract n_top words per topic using relevance (which determines what words are ordered by)\n",
    "    \"\"\"\n",
    "    topic_top_words = {}\n",
    "        \n",
    "    for idx, item in enumerate(relevance.as_matrix()):\n",
    "        top_words = [feature_names[i] for i in item.argsort()[:-n_top - 1:-1]]\n",
    "        topic_top_words[idx] = top_words\n",
    "    \n",
    "    return topic_top_words\n",
    "\n",
    "def topic_top_words(topic_term_dists, topic_num, feature_names, n_top):\n",
    "    \"\"\"\n",
    "    similar to above but returns topic info for single topic only\n",
    "    returns most frequent words for that topic\n",
    "    \"\"\"\n",
    "    topic_term_dist = topic_term_dists.loc[topic_num]\n",
    "    top_words = [feature_names[i] for i in topic_term_dist.argsort()[:-n_words - 1:-1]]\n",
    "    \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambdas = [1, .6, .2]\n",
    "topic_freq = calc_topic_freq(dtm, doc_topic_dists)\n",
    "n_top = 10\n",
    "\n",
    "top_words_dict = {}\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    relevance = get_relevance(topic_freq, topic_term_dists, lambda_)\n",
    "    top_words = get_top_words(relevance, feature_names, n_top)    \n",
    "    top_words_dict[lambda_] = top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate all topic info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create dictionary with all information of interest for each topic\n",
    "#get new order of topics\n",
    "topic_doc_dict = {}\n",
    "all_names = []\n",
    "\n",
    "num_papers = 10\n",
    "n_words = 15\n",
    "\n",
    "for ind, column in enumerate(doc_topic_dists.columns):\n",
    "\n",
    "    topic_num = order.index(ind) + 1\n",
    "    \n",
    "    #sort rows (documents) in descending order of probability of topic\n",
    "    doc_topic_dists = doc_topic_dists.sort_values([column], ascending = 0)\n",
    "    #get indexes of rows (correspond to documents) that have highest topic probability \n",
    "    indexes = doc_topic_dists.index.values\n",
    "    \n",
    "    #save ordered indexes of the top documents for the given topic\n",
    "    topic_doc_dict[topic_num] = [[indexes]]\n",
    "    \n",
    "    #get top words for all 3 lambda values\n",
    "    top_words_10 = top_words_dict[1][column]\n",
    "    top_words_06 = top_words_dict[.6][column]\n",
    "    top_words_02 = top_words_dict[.2][column]\n",
    "    topic_doc_dict[topic_num].append({1:top_words_10, .6:top_words_06, .2:top_words_02})\n",
    "    \n",
    "    #now loop through indexes and retrieve document information\n",
    "    titles, keywords, names = [], [], []\n",
    "    topic_names = {}\n",
    "    \n",
    "    top_n = []\n",
    "    \n",
    "    for i in indexes:\n",
    "        \n",
    "        #if don't have asked for N papers yet\n",
    "        if len(titles) < num_papers:\n",
    "        \n",
    "            #get paper ID and use that to retrieve remaining information\n",
    "            paper_id = publications_data.get_value(i, 'paper_id')\n",
    "            \n",
    "            #retrieve top N article titles + associated keywords\n",
    "            if len(top_n) < num_papers:\n",
    "                to_append = [publications_data.get_value(i, 'title'), doc_topic_dists.loc[i][column]]\n",
    "                for name in paper_id_to_authors[paper_id]:\n",
    "                    to_append.append(name)\n",
    "                top_n.append(to_append)\n",
    "            \n",
    "            to_add = False\n",
    "            for name in paper_id_to_authors[paper_id]:\n",
    "                if name in topic_names.keys():\n",
    "                    topic_names[name] += 1\n",
    "                else:\n",
    "                    topic_names[name] = 1\n",
    "                    names.append(name)\n",
    "\n",
    "                if topic_names[name] <= 2:\n",
    "                    to_add = True\n",
    "\n",
    "            if to_add:\n",
    "                titles.append([publications_data.get_value(i, 'title'), doc_topic_dists.loc[i][column]])\n",
    "                ak_keywords = publications_data.loc[publications_data['paper_id']==paper_id]['ak_keywords'].values[0]\n",
    "                if type(ak_keywords) != float:\n",
    "                    keywords += ak_keywords.split('; ')\n",
    "\n",
    "    #save all extracted information\n",
    "    topic_doc_dict[topic_num].append(titles)\n",
    "    topic_doc_dict[topic_num].append(list(set(keywords)))     \n",
    "    topic_doc_dict[topic_num].append(set(names))\n",
    "    \n",
    "    #also add top N articles (not constrained by what researchers they were associated with - this might be more appropriate for interpreting smaller topics)\n",
    "    #although it does not seem to make a great deal of difference\n",
    "    topic_doc_dict[topic_num].append(top_n)\n",
    "    \n",
    "    all_names.extend(names)\n",
    "    \n",
    "print('Number of researchers that have been listed in at least one set of top 10 papers: ', len(set(all_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print results of top 10 key article titles\n",
    "for key in sorted(topic_doc_dict.keys()):\n",
    "    if key <= num_topics_to_name:\n",
    "        data = topic_doc_dict[key]\n",
    "        print('Topic: ', key, \"\\n\")\n",
    "        \n",
    "        for key in data[1]:\n",
    "            print('Top words lambda ' + str(key) + ': ', *data[1][key], '\\n')\n",
    "\n",
    "        print(\"Article titles:\")\n",
    "        for i in data[2]:\n",
    "            print(i[0]) \n",
    "            print(i[1],'\\n')\n",
    "\n",
    "        print(\"AK keywords: \", *data[3], '\\n')\n",
    "\n",
    "        print(\"Researchers: \", *data[4], '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
