{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling (LDA) of Turing Institute publications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data manipulation and organisation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#topic modelling\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#visualisations\n",
    "import pyLDAvis\n",
    "from pyLDAvis import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#other\n",
    "import random, pkg_resources, os, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Require sklearn version 0.19, have: ' + pkg_resources.get_distribution(\"scikit-learn\").version)\n",
    "print(\"If have lower version, need to change 'n_components' to 'n_topics' when calling LatentDirichletAllocation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##might need to download nltk corpora and packages\n",
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Load data and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('data_files/final_dataset_full.csv'):\n",
    "    publications = pd.read_csv('data_files/final_dataset_full.csv')\n",
    "else:\n",
    "    publications_1 = pd.read_csv('data_files/final_dataset_1.csv')\n",
    "    publications_2 = pd.read_csv('data_files/final_dataset_2.csv')\n",
    "    publications = pd.concat([publications_1, publications_2])\n",
    "\n",
    "publications = publications.rename(columns={'full_name': 'name', 'current_uni': 'uni'})\n",
    "publications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check how many fellows have associated with each university\n",
    "uni_names = publications['uni'].unique()\n",
    "uni_fellows = publications.groupby('uni')['name'].unique()\n",
    "\n",
    "for i in range(len(uni_names)):\n",
    "    print(uni_names[i] + \": \" + str(len(uni_fellows[i])) + \" fellows\")\n",
    "    \n",
    "num_fellows = len(publications['name'].unique())\n",
    "print('\\nExcpect 138 fellows overall, have: ' + str(num_fellows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Note instances of multiple Turing fellows associated with 1 paper - remove duplicates based on paper ID (but keep note of all authors to attribute later)\n",
    "\n",
    "paper_id_to_authors dictionary keys correspond to all unique paper ids  \n",
    "dictionary value is a list of authors associated with that paper (most instances have 1 but in some cases have multiple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id_to_authors = {}\n",
    "for idx, row in publications.iterrows():\n",
    "    if row['paper_id'] not in paper_id_to_authors.keys():\n",
    "        paper_id_to_authors[row['paper_id']] = [row['name']]\n",
    "    else:\n",
    "        paper_id_to_authors[row['paper_id']].append(row['name'])\n",
    "        \n",
    "#drop duplicates - rename df\n",
    "publications_data = publications.drop_duplicates(subset = 'paper_id')\n",
    "\n",
    "#relabel index numbers\n",
    "publications_data = publications_data.reset_index(drop=True)\n",
    "\n",
    "print('Dataset contains {0[0]} unique articles'.format(publications_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create document-term matrix from text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1: create vector representation of vocabulary\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "\n",
    "#create document_term_matrix\n",
    "dtm = vectorizer.fit_transform(publications_data['full_text'].values.astype('U'))\n",
    "\n",
    "#retrieve word names at each vocabulary position\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 25\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = n_topics,  max_iter = 50, \n",
    "                                learning_method = 'online', random_state = 0) \n",
    "\n",
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise topics (LDAvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook\n",
    "prepared_data = pyLDAvis.sklearn.prepare(lda, dtm, vectorizer)\n",
    "pyLDAvis.display(prepared_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Organise LDA outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve topic_term and document_topic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_with_names(data, index_name, columns_name):\n",
    "    if type(data) == pd.DataFrame:\n",
    "        #we want our index to be numbered\n",
    "        df = pd.DataFrame(data.values)\n",
    "    else:\n",
    "        df = pd.DataFrame(data)\n",
    "    df.index.name = index_name\n",
    "    df.columns.name = columns_name\n",
    "    return df\n",
    "\n",
    "def series_with_name(data, name):\n",
    "    if type(data) == pd.Series:\n",
    "        data.name = name\n",
    "        #ensures a numeric index\n",
    "        return data.reset_index()[name]\n",
    "    else:\n",
    "        return pd.Series(data, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_term_dists = lda.components_ / lda.components_.sum(axis=1)[:, None]\n",
    "doc_topic_dists = lda.transform(dtm)\n",
    "\n",
    "topic_term_dists = df_with_names(topic_term_dists, 'topic', 'term')\n",
    "doc_topic_dists  = df_with_names(doc_topic_dists, 'doc', 'topic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5:  Format data for visualisation\n",
    "\n",
    "### Transform document_topic distribution to fellow_topic and institute_topic information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "publications_data df:  \n",
    "each row of the df corresponds to row of same index in doc_topic_dists   \n",
    "this allows us to get paper_id for the doc_topic_distribution of any paper   \n",
    "we can then match the paper_id against known Turing fellows associated with that paper  \n",
    "\n",
    "For each fellow, we get the average of distributions over topics for their documents and multiple by 100 to turn proportion into percentage -- this reflects the average percentage that their articles have been assigned each topic (which is based on how many words in each document have been assigned to each topic)      \n",
    "\n",
    "Overall topic importance is the average of above topic value assignment across all fellows  - this means each fellow contributes equally to the topic importance/size evaluation (even if they overall contributed fewer papers)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract topic proportions for each author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_names = publications_data['name'].unique()\n",
    "author_topic_dists = {}\n",
    "author_counts = {}\n",
    "    \n",
    "for index, row in publications_data.iterrows():\n",
    "    #most papers have 1 author but some have multiple so this assures paper topics are assigned to all relevant authors\n",
    "    for name in paper_id_to_authors[row['paper_id']]:\n",
    "        if name in author_topic_dists.keys():\n",
    "            author_topic_dists[name] += doc_topic_dists.iloc[index]\n",
    "            author_counts[name] += 1\n",
    "        else:\n",
    "            author_topic_dists[name] = doc_topic_dists.iloc[index]\n",
    "            author_counts[name] = 1\n",
    "\n",
    "# percentage of topics per author\n",
    "for name in author_topic_dists.keys():\n",
    "    if author_topic_dists[name].sum() != 0:\n",
    "        author_topic_dists[name] = author_topic_dists[name]/author_counts[name]*100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract overall institute topic \"importance\" (size) information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_importance, total = {}, 0\n",
    "for i in range(n_topics):\n",
    "    topic_importance[i] = 0\n",
    "    for name in author_topic_dists:\n",
    "        topic_importance[i] += author_topic_dists[name][i]\n",
    "    topic_importance[i] = topic_importance[i]/num_fellows\n",
    "    total += topic_importance[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all topic information in one df - save topic order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "author_topic_info = pd.DataFrame.from_dict(author_topic_dists, orient = 'columns')\n",
    "topic_info = pd.DataFrame.from_dict(topic_importance, orient='index')\n",
    "\n",
    "merged = pd.concat([author_topic_info, topic_info], axis =1 )\n",
    "merged = merged.rename(columns={0: 'topicVal'})\n",
    "\n",
    "##order topics by topicVal (overall topic importance)\n",
    "merged = merged.sort_values(['topicVal'], ascending = 0)\n",
    "\n",
    "topic_order = merged.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine topics that have assigned topic value < 1.5 (out of 100) into 1 topic that will be labeled 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_combine = merged.loc[merged['topicVal'] < 1.5]\n",
    "key_topics = merged.loc[merged['topicVal'] >= 1.5]\n",
    "key_topics = key_topics.append(to_combine.sum(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_topics.to_csv(\"visualisation/data_final.csv\", index_label='topicNum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Determine order in which to display researchers - ordered by university, by most prevalent topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order researchers by university + extract prevalent topic info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "researchers_dict = {}\n",
    "\n",
    "for column in merged:\n",
    "    \n",
    "    name = column\n",
    "    topics_order = merged.sort_values([column], ascending = 0)\n",
    "    topic_num = topics_order.index.values[0]\n",
    "    topic_val = topics_order[column][topic_num]\n",
    "    \n",
    "    if name != 'topicVal':\n",
    "\n",
    "        #use original df to search for uni in case some researchers where removed during duplicates deletion\n",
    "        uni = publications.loc[publications['name']== name]['uni'].values[0]\n",
    "\n",
    "        if uni not in researchers_dict.keys():\n",
    "            researchers_dict[uni] = {topic_num:[[name, topic_val]]}\n",
    "        else:\n",
    "            if topic_num not in researchers_dict[uni].keys():\n",
    "                researchers_dict[uni][topic_num] = [[name, topic_val]]\n",
    "            else:\n",
    "                researchers_dict[uni][topic_num].append([name, topic_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order researchers in descending order by topic (within each university)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_order\n",
    "\n",
    "#loop through universities\n",
    "researchers_order = []\n",
    "for i in sorted(researchers_dict.keys()):\n",
    "    topics = researchers_dict[i]\n",
    "    for j in topic_order:\n",
    "        if j in topics.keys():\n",
    "            researchers = topics[j]\n",
    "\n",
    "            if len(researchers) == 1:\n",
    "                researchers_order.append(researchers[0][0])\n",
    "            else:\n",
    "                researchers.sort(key=lambda x: x[1], reverse=True)\n",
    "                for k in researchers:\n",
    "                    researchers_order.append(k[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save author order to use in visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('visualisation/author_order_final.json', 'w') as fp:\n",
    "    json.dump(researchers_order, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Topic interpretation\n",
    "\n",
    "### Extract top documents for each topic (with associated top words and titles and AK API keywords)\n",
    "\n",
    "For each topic we extract top N words associated with that topic (this is very standard and is the same as displayed in the LDAvis visualisation)  \n",
    "\n",
    "Further, for each topic we get the titles of key articles associated with that topic - this makes topics easier to interpret   \n",
    "\n",
    "Each document has been assigned some proportion of each topic (with sum of all topic proportions within document = 1). For each topic, we ordered documents based on how much of that topic they were assigned. Then we looped through the documents extracting top 10 key article titles with the condition that if a researcher already contributed 2 papers to the top 10 key titles then we skipped their subsequent papers. This ensured that the top 10 key titles contained contributions of at least 5 researchers (as compared to trying to evaluate the meaning of a topic based on articles written by only one Turing fellow). This seemed reasonable given most topics were assigned to a number of researchers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get N top words for different values of lambda (1.0, 0.6, 0.2) -- from N most frequent to N most unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_topic_freq(dtm, doc_topic_dists):\n",
    "    \"\"\"\n",
    "    same as LDAvis, define topic frequency by proportion of words assigned to topic\n",
    "    \"\"\"\n",
    "    doc_lengths = dtm.sum(axis=1).getA1()\n",
    "    topic_freq = (doc_topic_dists.T * doc_lengths).T.sum()\n",
    "    \n",
    "    return topic_freq\n",
    "    \n",
    "def get_relevance(topic_freq, topic_term_dists, lambda_):\n",
    "    \"\"\"\n",
    "    function for calculating relevance\n",
    "    if lambda_ = 1 result is same as top N mot frequent words\n",
    "    if lambda_ = 0; returns list of top unique words for each topic \n",
    "    unique = not shared by other topics\n",
    "    \"\"\"\n",
    "    term_topic_freq = (topic_term_dists.T * topic_freq).T\n",
    "    term_frequency = np.sum(term_topic_freq, axis=0)\n",
    "    term_proportion = term_frequency / term_frequency.sum()  \n",
    "    \n",
    "    log_lift = np.log(topic_term_dists / term_proportion)\n",
    "    log_ttd = np.log(topic_term_dists)\n",
    "    relevance = lambda_ * log_ttd + (1 - lambda_) * log_lift\n",
    "    \n",
    "    return relevance\n",
    "\n",
    "def get_top_words(relevance, feature_names, n_top):\n",
    "    \"\"\"\n",
    "    function to extract n_top words per topic using relevance (which determines what words are ordered by)\n",
    "    \"\"\"\n",
    "    topic_top_words = {}\n",
    "        \n",
    "    for idx, item in enumerate(relevance.as_matrix()):\n",
    "        top_words = [feature_names[i] for i in item.argsort()[:-n_top - 1:-1]]\n",
    "        topic_top_words[idx] = top_words\n",
    "    \n",
    "    return topic_top_words\n",
    "\n",
    "def topic_top_words(topic_term_dists, topic_num, feature_names, n_top):\n",
    "    \"\"\"\n",
    "    similar to above but returns topic info for single topic only\n",
    "    returns most frequent words for that topic\n",
    "    \"\"\"\n",
    "    topic_term_dist = topic_term_dists.loc[topic_num]\n",
    "    top_words = [feature_names[i] for i in topic_term_dist.argsort()[:-n_words - 1:-1]]\n",
    "    \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [1, .6, .2]\n",
    "topic_freq = calc_topic_freq(dtm, doc_topic_dists)\n",
    "n_top = 10\n",
    "\n",
    "top_words_dict = {}\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    relevance = get_relevance(topic_freq, topic_term_dists, lambda_)\n",
    "    top_words = get_top_words(relevance, feature_names, n_top)    \n",
    "    top_words_dict[lambda_] = top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate all topic info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create dictionary with all information of interest for each topic\n",
    "#get new order of topics\n",
    "topic_doc_dict = {}\n",
    "all_names = []\n",
    "\n",
    "num_papers = 10\n",
    "n_words = 15\n",
    "\n",
    "#loop through topics\n",
    "for column in doc_topic_dists:\n",
    "\n",
    "    #order docs by how much topic assigned + save indexes of top docs\n",
    "    doc_topic_dists = doc_topic_dists.sort_values([column], ascending = 0)\n",
    "    indexes = doc_topic_dists.index.values\n",
    "    topic_doc_dict[column] = [[indexes]]\n",
    "    \n",
    "    #get top words for all 3 lambda values\n",
    "    top_words_10 = top_words_dict[1][column]\n",
    "    top_words_06 = top_words_dict[.6][column]\n",
    "    top_words_02 = top_words_dict[.2][column]\n",
    "    topic_doc_dict[column].append({1:top_words_10, .6:top_words_06, .2:top_words_02})\n",
    "    \n",
    "    #loop through indexes and retrieve document information\n",
    "    titles, keywords, names = [], [], []\n",
    "    topic_names = {}\n",
    "    top_n = []\n",
    "    \n",
    "    for i in indexes:\n",
    "        \n",
    "        #if don't have asked for N papers yet\n",
    "        if len(titles) < num_papers:\n",
    "        \n",
    "            #get paper ID and use that to retrieve remaining information\n",
    "            paper_id = publications_data.get_value(i, 'paper_id')\n",
    "            \n",
    "            #retrieve top N article titles + associated keywords\n",
    "            if len(top_n) < num_papers:\n",
    "                to_append = [publications_data.get_value(i, 'title'), doc_topic_dists.loc[i][column]]\n",
    "                for name in paper_id_to_authors[paper_id]:\n",
    "                    to_append.append(name)\n",
    "                top_n.append(to_append)\n",
    "            \n",
    "            to_add = False\n",
    "            for name in paper_id_to_authors[paper_id]:\n",
    "                if name in topic_names.keys():\n",
    "                    topic_names[name] += 1\n",
    "                else:\n",
    "                    topic_names[name] = 1\n",
    "                    names.append(name)\n",
    "\n",
    "                if topic_names[name] <= 2:\n",
    "                    to_add = True\n",
    "\n",
    "            if to_add:\n",
    "                titles.append([publications_data.get_value(i, 'title'), doc_topic_dists.loc[i][column]])\n",
    "                ak_keywords = publications_data.loc[publications_data['paper_id']==paper_id]['ak_keywords'].values[0]\n",
    "                if type(ak_keywords) != float:\n",
    "                    keywords += ak_keywords.split('; ')\n",
    "\n",
    "    #save all extracted information\n",
    "    topic_doc_dict[column].append(titles)\n",
    "    topic_doc_dict[column].append(list(set(keywords)))     \n",
    "    topic_doc_dict[column].append(set(names))\n",
    "    \n",
    "    all_names.extend(names)\n",
    "    \n",
    "print('Number of researchers that have been listed in at least one set of top 10 papers: ', len(set(all_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print results of top 10 key article titles\n",
    "n = 1\n",
    "for topic in topic_order:\n",
    "    data = topic_doc_dict[topic]\n",
    "    print('Topic: ', n, \"\\n\")\n",
    "\n",
    "    for key in data[1]:\n",
    "        print('Top words lambda ' + str(key) + ': ', *data[1][key], '\\n')\n",
    "\n",
    "    print(\"Article titles:\")\n",
    "    for i in data[2]:\n",
    "        print(i[0]) \n",
    "        print(i[1],'\\n')\n",
    "\n",
    "    print(\"AK keywords: \", *data[3], '\\n')\n",
    "\n",
    "    print(\"Researchers: \", *data[4], '\\n')\n",
    "    \n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTHER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-order merged, doc_topic and topic_term distributions by overall topic size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topic_order(dtm, doc_topic_dists):\n",
    "    \"\"\"\n",
    "    function which returns same topic order as LDAvis\n",
    "    \"\"\"\n",
    "    doc_lengths      = series_with_name(dtm.sum(axis=1).getA1(), 'doc_length')\n",
    "    topic_freq       = (doc_topic_dists.T * doc_lengths).T.sum()\n",
    "    topic_proportion = (topic_freq / topic_freq.sum()).sort_values(ascending=False)\n",
    "    \n",
    "    return topic_proportion.index \n",
    "\n",
    "def set_topic_order(topic_term_dists, doc_topic_dists, topic_order):\n",
    "    \n",
    "    topic_term_dists = topic_term_dists.ix[topic_order]\n",
    "    doc_topic_dists  = doc_topic_dists[topic_order]\n",
    "    \n",
    "    return topic_term_dists, doc_topic_dists\n",
    "\n",
    "def rename_topic_order(topic_term_dists, doc_topic_dists):\n",
    "    \n",
    "    topic_term_dists = topic_term_dists.reset_index(drop=True)\n",
    "    topic_term_dists.index.name = 'topic'\n",
    "    \n",
    "    doc_topic_dists.columns = [i for i in range(25)]\n",
    "    doc_topic_dists.columns.name = 'topic'\n",
    "    \n",
    "    return topic_term_dists, doc_topic_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_order = merged.index.values\n",
    "\n",
    "topic_term_dists, doc_topic_dists = set_topic_order(topic_term_dists, doc_topic_dists, topic_order)\n",
    "topic_term_dists, doc_topic_dists = rename_topic_order(topic_term_dists, doc_topic_dists)\n",
    "\n",
    "merged = merged.reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
