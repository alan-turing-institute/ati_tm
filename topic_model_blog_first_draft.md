# Title

The Alan Turing Institute provides a platform for collaboration across a range of traditional research disciplines (for example, mathematics, computer science, the social sciences), bringing together leading UK Universities. This range is a real strength of the Institute that makes it well equipped to contribute to shaping the still new and emerging field of data science. The associated challenge is that it is not trivial for Turing fellows (and others) to find out what each fellow is working on, who is working on similar topics (especially if traditionally they work in a different discipline) and, consequently, who it might be best to approach about possible collaborations. One of the questions even we as a team hear the most is; What exactly do you do? The idea behind this project was to "use data science" to try to address this question. For each Turing fellow we collected their recent publications and extracted the topics covered in these publications. This allowed us to explore the breadth of expertise represented by the Institute as well as identify overlaps and similarities between Turing fellows.

### Data collection

Possibly the greatest challenge, as is true of many data science projects, was creating a comprehensive, valid and reliable dataset. An ideal dataset would be one where (i) for each fellow we had a record of their recent articles, (ii) each identified article was written by that fellow and (iii) the process of collecting the data could be reproduced by anyone (accepting constraints such as updates and changes to databases etc.). This is less straightforward than might seem as keyword searches in databases often return similar results not just exact matches, most comprehensive databases are behind a paywall, individual Fellows rely on a wide range of platforms for storing publication records and we wanted to minimise manual searching on the web (time consuming and hard to replicate). We tried to satisfy the requirements as much as possible, with a particular focus on validity, using only open access routes.

The Turing Institute website provides a list of all articles published since the start of the Institute. However, articles in this set only provide a few data points for some researchers and no data points at all for others. So even though the data is valid and reliable, it is far from comprehensive. This is not surprising given the Institute is still fairly new and many researchers have joined only recently. Instead, we used Microsoft’s [Academic Knowledge (AK) API](https://docs.microsoft.com/en-us/azure/cognitive-services/academic-knowledge/home) which is a free service for searching academic publications. Firstly, we manually collected and verified IDs under which each Turing fellow is stored in the AK database (to ensure validity). We then used the IDs to get a list of up to 50 publications for each Turing fellow (published in 2012 or later). The AK database returns article title and in some instances also article abstract, DOI and a list of source URLs. Source URLs provide links to a range of content from only the article abstract to full text versions of the article.

The returned source URLs provided a means of downloading at least a subset of the identified articles. Further, we used the article DOI (where available) to extract a further set of source links from the [oaDOI API](https://oadoi.org) (a repository of links to full text versions of open access articles). The articles were downloaded as PDFs and converted to text files using Contentmine's [NORMA](https://github.com/ContentMine/norma) tool. The final dataset consists of more than 1400 full text articles. Some Turing fellows are better represented than others due to a range of factors such as having more complete records in the AK database or having more publications suitable to web scraping (automated data collection from the web).

### Topic modelling

In terms of typical text data analysis problems, a collection of 1400 documents is not considered a large dataset. However, it is clear that even without requiring to read the entire article, manually searching through each of the downloaded texts for keywords, for example, would be extremely time consuming and inefficient. Topic modelling is a statistical approach to discovering a set of ‘topics’ in a collection of documents by analysing the words within the texts. The general assumption behind topic modelling is that documents about the same topic use certain words more frequently than others. Even when a document covers a number of topics (which is frequently the case), we can exploit frequency and co-occurrence of words within a set of texts to extract the underlying topics.

The most commonly used topic modelling method is Latent Dirichlet Allocation (LDA). LDA is a probabilistic generative model; it assumes a generative process that describes how documents are written given a collection of topics and then tries to reverse that process i.e., infer topics that could have generated a given set of documents. To understand the generative process, lets imagine we have 2 topics: the Czech Republic and Great Britain, and we want to write a document that is mostly about the Czech Republic but that also compares it to Britain. Further, we have 4 words in our vocabulary: British, Czech, weather, beer. Each topic is defined by the probability of each of the 4 words occurring in that topic and the document is defined by the proportion that refers to each topic. For example:   

 *Topic Britain : British  = .5, Czech = .0, weather = .4, beer = .1*  
 *Topic Czech Rep.: British = .0, Czech = .5, weather = .2, beer = .3*

*Document : Britain = .2, Czech Rep. = .8*

Lets say that I want to write 10 words of the above defined document. For each word that I write:  
(i) I randomly choose a topic (and I should choose Topic Czech Rep. 80% of the time)  
(ii) I randomly choose a word from that topic  

Following this two-step process for each word, I might generate something like:   

*Czech beer Czech beer Czech beer Czech weather British weather.*

LDA tries to infer the original topics from the generated document. More formally, each topic is a multinomial distribution over the corpus vocabulary (list of all unique words in the analysed texts) and each document is returned as a multinomial distribution over all topics. LDA thus provides a means of identifying a set of research topics covered in the downloaded articles as well as quantifying the proportion of each article that is about any of the identified research topics.

The returned topics require manual labelling for interpretation. Most commonly this is accomplished by looking at the top (usually most frequent or high probability) N words in each topic. The most frequent 5 words of a topic might be 'prior', 'distribution', 'posterior', 'sampling' and 'inference' and we would label that topic as 'Bayesian statistics’. However, many of the most frequent words in a topic are also the most frequent words in all texts and appear in many topics. In our case these were words such as 'model'. We therefore also looked at words that were not necessarily the most frequent but that were unique to a topic. Lastly, we also extracted the titles of articles that were assigned the highest proportion of a given topic. Especially the last step helped to interpret the returned topics as it provided context for the topic words.

### Results and Visualisation

The goal of the project was to produce a visualisation that would make it easy to summarise and compare the work of all Turing fellows. This meant that we had to meaningfully visualise data for almost 100 separate entities displaying a high number of categorical data points within each (as many as the number of topics). That is a large quantity of unordered information to place within a single visualisation. Topic visualisation remains an active area of research but existing methods primarily focus on displaying the content of topics, less of a priority for us. Our key aim was to visualise each fellow individually, displaying what topics they researched in a manner that allowed for direct comparisons with other Turing fellows while simultaneously saying something about the entire Institute.

We applied LDA to the downloaded research articles, modelling 25 topics in the collection. As one of its results, LDA identifies what proportion of each document was assigned to which of the modelled topics. For each fellow, we averaged these proportions over all of their articles. We also averaged the topic proportions across all Turing fellows. For each topic we thus obtained (i) the proportion of each fellow's research that is about that topic and (ii) the proportion of the entire Institute's research that touches on that topic (overall topic size).

The left panel of the visualisation lists all the identified topics, ordered by overall topic size. It is clear that the top topics are more generic and as topic size decreases, the topics get more specific. A common feature of LDA models is that the smaller topics are hard to interpret and often only refer to a very small subset of the analysed texts. These topics were combined into a single topic labeled 'other' leaving 14 topics (out of original 25) in the visualisation.

As mentioned in the previous section, topic labels were based on top words (most frequent and most unique) returned by LDA as well as characteristics of articles (title and associated keywords) that had the highest proportion of that topic. For example,...
<!-- Here have a description of how topics were labeled using single specific example -->

We settled on an adaptation of the [aster plot](http://bl.ocks.org/bbest/2de0e25d4840c68f2db1) for visualising each Turing fellow. Each segment represents a topic and its width corresponds to overall topic size (the order is the same as for topics in the left panel). The coloured segments for each fellow represent their specific topic distribution. Unlike the original aster plot implementation, we used area not radius to represent individual topic size for each fellow. In other words, each fellow has the same overall coloured area that is divided between their topics based on the identified individual topic proportions. The visualisation clearly shows that whereas some fellows work primarily in a small number of specialised areas, some fellows' work is split fairly evenly among a large number of topics.

The fellows are ordered by the University they belong to. Further, for each fellow we identified their primary topic (topic with highest proportion within their articles). Within each University column, we ordered fellows according to their primary topics, using same order as for topics in the left column (i.e., from largest to smallest overall topic size). For example, Thomas Lukasiewicz and Ian Horrocks are at the bottom of the Oxford column as their primary topic is Knowledge representation (ranked as 10th among all topics), and they are placed right after Mihaela van der Schaar whose primary topic is Networks (9th topic).

Lastly, some of the fellows are greyed out. These are researchers for whom we did not download a sufficient number of articles. Note that this is not a reference to how many articles they might have in reality, it is merely a reflection of how many articles we successfully obtained. Even though their articles were kept in the dataset and contributed to the topic model, we wanted to highlight that their results might be less representative.

![](visualisation/turing_fellows_topics.png)


All code can be found on [GitHub](https://github.com/alan-turing-institute/ati_tm)
