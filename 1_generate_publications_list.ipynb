{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract publication information from the AK API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Microsoft's Academic Knowledge (AK) API provides a free database of academic publications. In a previous step we colected a list of IDs under which the Turing fellows are stored in the AK database (see turing_AK_IDs.csv). Here we used the collected IDs to extract up to 50 articles published by each fellow in 2012 or later.  \n",
    "\n",
    "To use the AK database, an access key needs to be obtained. Here we load it from api_key.txt.  \n",
    "\n",
    "Results were stored as publications_eng.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import http.client, urllib.parse, json, re, unicodedata, string\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "(i) read key for AK API from text file  \n",
    "(ii) load information about AK API ID for each fellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract access key to academic knowledge api\n",
    "with open('api_key.txt', 'r') as myfile:\n",
    "    key = myfile.read().replace('\\n', '')\n",
    "\n",
    "ids_list = pd.read_csv('data_files/turing_AK_IDs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function for saving generated data to csv file\n",
    "directory = 'data_files/' \n",
    "\n",
    "def save_to_file(my_list, file_name):\n",
    "    \"\"\"\n",
    "    function for saving data to file\n",
    "    input is list which is saved as one row in file called file_name\n",
    "    \"\"\"\n",
    "    f = open(directory + file_name, 'a') \n",
    "    for i in my_list:\n",
    "        f.write(str(i) + ',')\n",
    "    f.write('\\n') # write a line ending\n",
    "    f.close() #close and \"save\" the output file\n",
    "      \n",
    "def save_info(data, file_name):\n",
    "    \"\"\"\n",
    "    save data (information) returned by the id_to_paper function\n",
    "    - function returns list of dictionaries\n",
    "    - can iterate through list\n",
    "    \n",
    "    file_name = where want to save extracted info\n",
    "    \"\"\" \n",
    "    for i in data:\n",
    "        article = []\n",
    "        \n",
    "        for j in i.keys():\n",
    "\n",
    "            if j != 'keywords' and j != 'urls':\n",
    "\n",
    "                #remove commas\n",
    "                text = str(i[j]).replace(\",\", \"\")\n",
    "\n",
    "                #remove accents\n",
    "                text = strip_accents(text)\n",
    "\n",
    "                #append retrieved info to article list\n",
    "                article.append(text)\n",
    "\n",
    "            else:\n",
    "                article.extend(i[j])\n",
    "\n",
    "        #save all article related information\n",
    "        save_to_file(article, file_name)\n",
    "            \n",
    "#function for removing accents from strings\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def count_found(turing_fellows):\n",
    "    count = 0\n",
    "    for i in turing_fellows:\n",
    "        if turing_fellows[i] != []:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for extracting publications data from AK API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_abstract(IA):\n",
    "    \"\"\"\n",
    "    Function for creating abstract out of abstract indexes provided by academic knowledge API\n",
    "    \n",
    "    IA is a dictionary with keywords IndexLength and InvertedIndex \n",
    "    \n",
    "    IA.IndexLength = length of abstract\n",
    "    \n",
    "    IA.InvertedIndex is a dictionary where each word is a key and it provides an associated list of indexes\n",
    "    of where in the abstract it appears\n",
    "    \n",
    "    returns abstract as string\n",
    "    \"\"\"\n",
    "    \n",
    "    #create list of words of length n where n = length of abstract\n",
    "    abstract = [\"word\"] * IA['IndexLength']\n",
    "\n",
    "    #search through each unique word in the abstract\n",
    "    for word in IA['InvertedIndex'].keys():\n",
    "        \n",
    "        #find positions/indexes in the abstract that this word appears at (can be 1 position or multiple)\n",
    "        for index in IA['InvertedIndex'][word]:\n",
    "        \n",
    "            #place word in correct position in abstract list\n",
    "            abstract[index] = word;\n",
    "\n",
    "    #join all abstract words and return as string\n",
    "    return ' '.join(abstract)\n",
    "\n",
    "def get_parameters(author_id, year, count):\n",
    "    #format expression\n",
    "    expression = \"AND(Composite(AA.AuId=\" + str(author_id) + \"),Y>\" + str(year) + \")\"\n",
    "    \n",
    "    #define attributes that want to extract\n",
    "    att_list = ['Id', 'L', 'AA.AuN', 'AA.AuId', 'AA.AfN','Ti', 'F.FN', 'J.JN', 'E']\n",
    "    attributes = \",\".join(att_list)\n",
    "    \n",
    "    params = urllib.parse.urlencode({\n",
    "        'expr': expression, \n",
    "        'model': 'latest', #unless want to use a different version of the AK API\n",
    "        'count': str(count), #number of results to return\n",
    "        'offset': '0', #index of the first result to retun\n",
    "        'orderby': 'Y:asc', #name of attribute used for sorting  \n",
    "        'attributes': attributes,\n",
    "    })\n",
    "    \n",
    "    return params\n",
    "\n",
    "def get_data(author_id, data_dict):      \n",
    "    #all information of interest is saved under the key 'entities'\n",
    "    #data_dict['entities'] returns a list of length = number of articles asked for\n",
    "    #within that list, at each index is a dictionary with all the information we want to access about that article\n",
    "    articles = data_dict[\"entities\"]\n",
    "    final_articles_data = []\n",
    "    \n",
    "    #loop through all articles:\n",
    "    for i in range(len(articles)):\n",
    "        \n",
    "        entities = articles[i]\n",
    "        \n",
    "        name, affiliation, paper_id = \"\", \"\", \"\"\n",
    "        language, title, journal_name = \"\", \"\", \"\", \n",
    "        abstract, doi = \"\", \"\"\n",
    "        field_of_study, urls = [], []\n",
    "        \n",
    "        #author information\n",
    "        if 'AA' in entities.keys():\n",
    "            for i in entities[\"AA\"]:\n",
    "                #find right author in list of paper authors\n",
    "                if i['AuId'] == author_id:\n",
    "                    name = i['AuN']\n",
    "                    name = name.replace(\"Å‚\", \"l\")\n",
    "                    #sometimes affiliation info is missing\n",
    "                    if 'AfN' in i.keys():\n",
    "                        affiliation = i['AfN']\n",
    "\n",
    "        #paper ID\n",
    "        if 'Id' in entities.keys():\n",
    "            paper_id = entities['Id']\n",
    "  \n",
    "        #paper language\n",
    "        if 'L' in entities.keys():\n",
    "            language = entities['L']\n",
    "\n",
    "        #article title\n",
    "        if 'Ti' in entities.keys():\n",
    "            title = entities[\"Ti\"]\n",
    "\n",
    "        #journal name\n",
    "        if 'J.JN' in entities.keys():\n",
    "            journal_name = entities['J.JN']\n",
    "\n",
    "        #fields of study (F) return a list where at each index is a dictionary of the form {\"FN\":{name of field of study}}\n",
    "        #each article has different number of field keywords associated with it\n",
    "        if \"F\" in entities.keys():\n",
    "            for i in range(len(entities['F'])):\n",
    "                field_of_study.append(entities[\"F\"][i][\"FN\"])\n",
    "\n",
    "        #entities[\"E\"] returns a string of a dictionary - use JSON to interpret as dictionary   \n",
    "        #we are interested in IA \n",
    "        #IA is a dictionary with the keys IndexLength and InvertedIndex\n",
    "        #abstract is in \"inverted form\" so need to recontruct it\n",
    "        if \"E\" in entities.keys():\n",
    "            E = json.loads(entities[\"E\"])\n",
    "            if 'IA' in E.keys():\n",
    "                IA = E[\"IA\"]\n",
    "                abstract = generate_abstract(IA)\n",
    "                abstract = abstract.rstrip()\n",
    "                abstract = re.sub('[^A-Za-z0-9]+', ' ', abstract)\n",
    "\n",
    "            #get urls to source sites - can use to try to get full pdf\n",
    "            if 'S' in E.keys():\n",
    "                source = E['S']\n",
    "                for i in source:\n",
    "                    urls.append(i['U'])\n",
    "                #make sure the array is always 20 in length\n",
    "                for num in range(20-len(urls)):\n",
    "                    urls.append(\"\")\n",
    "\n",
    "            if 'DOI' in E.keys():\n",
    "                doi = E['DOI']\n",
    "                \n",
    "        #save each article's information if article in English\n",
    "        if language == 'en':\n",
    "            final_articles_data.append({'author_id':author_id, 'name':name, 'affiliation':affiliation,  'paper_id': paper_id, \n",
    "                    'title': title, 'abstract': abstract, 'journal_name':journal_name, 'doi':doi, 'urls':urls, \n",
    "                    'keywords':field_of_study})\n",
    "\n",
    "    return final_articles_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id_to_paper(author_id, key, year = 2011, count = 20):\n",
    "    \"\"\"\n",
    "    Function for extracting publication/article information\n",
    "    \n",
    "    Need to provide:\n",
    "    - author ID\n",
    "    - cut-off year for publications that interested in (this year is NOT included) i.e. for 2017 put 2016\n",
    "    - count = how many articles want to return\n",
    "    \n",
    "    Accepted attributes to return:\n",
    "    - Id = unique article ID (can use to catch possible repeats)\n",
    "    - Ti = article title\n",
    "    - F.FN = Field of study keywords, from Microsoft:\n",
    "        Our fields of study are formal research topics that should be recognized by the broader research community. \n",
    "        We currently have around 50k of them, and are looking to expand them.\n",
    "        We start with author and publisher supplied keywords and attempt to conflate them with our fields of study. \n",
    "        If the conflation is of sufficient quality we expose the labels via our API.\n",
    "        We also leverage machine learning techniques to do additional labeling.    \n",
    "    - J.JN = journal name\n",
    "    - L = paper language (english = eng)\n",
    "    - E = returns a JSON object with further information such as the inverted abstract.\n",
    "        Attributes of interest in the E object: IA = inverted abstract, IA.IndexLength - abstract word count, \n",
    "        IA.InvertedIndex = Dictionary of unique abstract words and their corresponding positions in the original abstract (saved as list)\n",
    "        S = Sources - list of web sources of the paper; S.Ty = Source Type (1:HTML, 2:Text, 3:PDF, 4:DOC, 5:PPT, 6:XLS, 7:PS); S.U = source url\n",
    "    - AA = further author information, to confirm name and affiliation\n",
    "    \n",
    "    returns list of dictionaries, one dictionary per article with all info specified in attributes list + supplied info\n",
    "    \"\"\"\n",
    "    \n",
    "    #required packages to load\n",
    "    #import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n",
    "    \n",
    "    headers = {'Ocp-Apim-Subscription-Key': key}\n",
    "\n",
    "    #set parameters\n",
    "    params = get_parameters(author_id, year, count)\n",
    "\n",
    "    try:\n",
    "        conn = http.client.HTTPSConnection('westus.api.cognitive.microsoft.com')\n",
    "        conn.request(\"GET\", \"/academic/v1.0/evaluate?%s\" % params, \"{body}\", headers)\n",
    "        response = conn.getresponse()\n",
    "    \n",
    "        final_articles_data = []\n",
    "    \n",
    "        #save response from query, this returns bytes object\n",
    "        data = response.read()\n",
    "        #convert bytes to string\n",
    "        #data = data.decode(\"utf-8\") \n",
    "        \n",
    "        try:\n",
    "            data_dict=json.loads(data)\n",
    "        except ValueError:\n",
    "            data_dict = {}\n",
    "\n",
    "        #check that something has been returned, if not - ignore\n",
    "        if 'entities' in data_dict.keys():\n",
    "            \n",
    "            final_articles_data = get_data(author_id, data_dict)\n",
    "            \n",
    "        #close connection and return information on all found articles\n",
    "        conn.close()\n",
    "        return final_articles_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Extract publications information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file where extract publications per author\n",
    "file_name = 'publications_eng.csv'\n",
    "#number of publications to retrieve\n",
    "num_publications = 50\n",
    "\n",
    "#create headings in file\n",
    "#anticipate up to 20 URLs and 50 keywords (this is excessive but avoids possible errors - will delete empty columns later)\n",
    "num_urls = 20\n",
    "attributes = ['author_id', 'name','affiliation','paper_id','title','abstract', 'journal_name', 'doi']\n",
    "for i in range(num_urls):\n",
    "    attributes.append('url_' + str(i))\n",
    "num_keywords = 50\n",
    "for i in range(num_keywords):\n",
    "    attributes.append('keyword_' + str(i))\n",
    "\n",
    "# save_to_file(attributes, file_name)    \n",
    "\n",
    "failed, tries = [], []\n",
    "count = ids_list.shape[0]\n",
    "\n",
    "for index, row in ids_list.iterrows():\n",
    "    \n",
    "    if index%(int(count/10)) == 0:\n",
    "        print(str(round(index/count,2)*100)+\"%\")\n",
    "        \n",
    "    #some fellows have multiple IDs\n",
    "    author_ids = row['id_1':'id_3']\n",
    "\n",
    "    for author_id in author_ids:\n",
    "        if not pd.isnull(author_id):\n",
    "            max_tries = 10\n",
    "            num_tries, found = 0, 0\n",
    "\n",
    "            while num_tries < max_tries and found == 0:\n",
    "                num_tries += 1\n",
    "                data = id_to_paper(int(author_id), key,  count = num_publications)\n",
    "                if data != []:\n",
    "                    found = 1\n",
    "\n",
    "            tries.append(num_tries)\n",
    "            save_info(data, file_name)\n",
    "\n",
    "            if found == 0:\n",
    "                failed.append(author_id)\n",
    "                \n",
    "print('end')\n",
    "print(\" \")\n",
    "print('Failed to find authors: ' + str(failed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
